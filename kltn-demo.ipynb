{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7440574,"sourceType":"datasetVersion","datasetId":4330645},{"sourceId":7479698,"sourceType":"datasetVersion","datasetId":4353928}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-26T00:44:09.623508Z","iopub.execute_input":"2024-01-26T00:44:09.624523Z","iopub.status.idle":"2024-01-26T00:44:10.066880Z","shell.execute_reply.started":"2024-01-26T00:44:09.624485Z","shell.execute_reply":"2024-01-26T00:44:10.065809Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/model-demo/pytorch_model_distilbert-base-vi-cased_dtv3.bin\n/kaggle/input/idioms-data/README.md\n/kaggle/input/idioms-data/ViID_dict.txt\n/kaggle/input/idioms-data/data_ver2/dev.json\n/kaggle/input/idioms-data/data_ver2/train.json\n/kaggle/input/idioms-data/data_ver2/test.json\n/kaggle/input/idioms-data/data_ver1/dev.json\n/kaggle/input/idioms-data/data_ver1/train.json\n/kaggle/input/idioms-data/data_ver1/test.json\n/kaggle/input/idioms-data/pytorch_pretrained_bert/modeling.py\n/kaggle/input/idioms-data/pytorch_pretrained_bert/file_utils.py\n/kaggle/input/idioms-data/pytorch_pretrained_bert/tokenization.py\n/kaggle/input/idioms-data/pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py\n/kaggle/input/idioms-data/pytorch_pretrained_bert/optimization.py\n/kaggle/input/idioms-data/pytorch_pretrained_bert/__init__.py\n/kaggle/input/idioms-data/pytorch_pretrained_bert/__main__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://huggingface.co/Geotrend/distilbert-base-vi-cased","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:10.068963Z","iopub.execute_input":"2024-01-26T00:44:10.069376Z","iopub.status.idle":"2024-01-26T00:44:15.278668Z","shell.execute_reply.started":"2024-01-26T00:44:10.069349Z","shell.execute_reply":"2024-01-26T00:44:15.277278Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'distilbert-base-vi-cased'...\nremote: Enumerating objects: 14, done.\u001b[K\nremote: Counting objects: 100% (4/4), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 10\u001b[K\nUnpacking objects: 100% (14/14), 58.66 KiB | 6.52 MiB/s, done.\nFiltering content: 100% (2/2), 434.86 MiB | 123.62 MiB/s, done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%cd '/kaggle/input/idioms-data'","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:15.280477Z","iopub.execute_input":"2024-01-26T00:44:15.280849Z","iopub.status.idle":"2024-01-26T00:44:15.288743Z","shell.execute_reply.started":"2024-01-26T00:44:15.280815Z","shell.execute_reply":"2024-01-26T00:44:15.287686Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/idioms-data\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\nidiom_vocab = eval(open('/kaggle/input/idioms-data/ViID_dict.txt').readline())\nidiom_vocab = {each: i for i, each in enumerate(idiom_vocab)}","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.202222Z","iopub.execute_input":"2024-01-26T00:44:21.202698Z","iopub.status.idle":"2024-01-26T00:44:21.215102Z","shell.execute_reply.started":"2024-01-26T00:44:21.202667Z","shell.execute_reply":"2024-01-26T00:44:21.214127Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import argparse\nimport collections\nimport logging\nimport json\nimport math\nimport os\nimport random\nimport pickle\nfrom tqdm import tqdm, trange\nimport re\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoTokenizer\n\nfrom pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\nfrom pytorch_pretrained_bert.modeling import PreTrainedBertModel, BertModel, BertConfig, BertForMultipleChoice\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:15.291353Z","iopub.execute_input":"2024-01-26T00:44:15.291752Z","iopub.status.idle":"2024-01-26T00:44:21.201025Z","shell.execute_reply.started":"2024-01-26T00:44:15.291707Z","shell.execute_reply":"2024-01-26T00:44:21.199995Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class ViidExample(object):\n    def __init__(self,\n                 tag,\n                 context,\n                 options,\n                 label=None):\n        self.tag = tag\n        self.context = context\n        self.options = options\n        self.label = label\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = \"\"\n        s += \"tag: %s\" % (self.tag)\n        s += \", context: %s\" % (self.context)\n        s += \", options: [%s]\" % (\", \".join(self.options))\n        if self.label is not None:\n            s += \", answer: %s\" % self.options[self.label]\n        return s","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.216906Z","iopub.execute_input":"2024-01-26T00:44:21.217327Z","iopub.status.idle":"2024-01-26T00:44:21.225152Z","shell.execute_reply.started":"2024-01-26T00:44:21.217290Z","shell.execute_reply":"2024-01-26T00:44:21.224209Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self,\n                 tag,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 option_ids,\n                 position,\n                 label):\n        self.tag = tag # use int number\n        self.input_ids = input_ids # [max_seq_length]\n        self.input_mask = input_mask # [max_seq_length]\n        self.segment_ids = segment_ids # [max_seq_length]\n        self.option_ids = option_ids # [num_choices]\n        self.position = position\n        self.label = label","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.226382Z","iopub.execute_input":"2024-01-26T00:44:21.226733Z","iopub.status.idle":"2024-01-26T00:44:21.237629Z","shell.execute_reply.started":"2024-01-26T00:44:21.226707Z","shell.execute_reply":"2024-01-26T00:44:21.236646Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"max_seq_length = 256\ndoc_stride = 128\nmax_query_length = 128\ndo_train = True\ndo_predict = True\ntrain_batch_size = 32\npredict_batch_size = 64\nlearning_rate = 2*10**-5\nnum_train_epochs = 10\nwarmup_proportion = 0.1\nn_best_size = 20\nmax_answer_length = 32\nverbose_logging = False\n\nno_cuda = False\nseed = 42\ngradient_accumulation_steps = 1\ndo_lower_case = True\nlocal_rank = -1\nfp16 = False\nloss_scale = 0","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.238933Z","iopub.execute_input":"2024-01-26T00:44:21.239340Z","iopub.status.idle":"2024-01-26T00:44:21.249291Z","shell.execute_reply.started":"2024-01-26T00:44:21.239302Z","shell.execute_reply":"2024-01-26T00:44:21.248219Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if local_rank == -1 or no_cuda:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n    n_gpu = torch.cuda.device_count()\nelse:\n    torch.cuda.set_device(local_rank)\n    device = torch.device(\"cuda\", local_rank)\n    n_gpu = 1\n    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n    torch.distributed.init_process_group(backend='nccl')\n    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n    device, n_gpu, bool(local_rank != -1), fp16))\n\nif gradient_accumulation_steps < 1:\n    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n                        gradient_accumulation_steps))\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif n_gpu > 0:\n    torch.cuda.manual_seed_all(seed)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.250596Z","iopub.execute_input":"2024-01-26T00:44:21.250993Z","iopub.status.idle":"2024-01-26T00:44:21.317504Z","shell.execute_reply.started":"2024-01-26T00:44:21.250956Z","shell.execute_reply":"2024-01-26T00:44:21.316537Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_choice(idiom):\n      return ''.join([s.lower() for s in idiom])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.321458Z","iopub.execute_input":"2024-01-26T00:44:21.321818Z","iopub.status.idle":"2024-01-26T00:44:21.326270Z","shell.execute_reply.started":"2024-01-26T00:44:21.321789Z","shell.execute_reply":"2024-01-26T00:44:21.325296Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class BertForCloze(PreTrainedBertModel):\n    \"\"\"BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It's the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `option_ids`: a torch.LongTensor of shape [batch_size, num_choices]\n        `positions`: a torch.LongTensor of shape [batch_size]\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n    \"\"\"\n    def __init__(self, config, num_choices):\n        super(BertForCloze, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.idiom_embedding = nn.Embedding(len(idiom_vocab), config.hidden_size)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, option_ids, token_type_ids, attention_mask, positions, labels=None):\n\n        encoded_layer, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        blank_states = encoded_layer[[i for i in range(len(positions))], positions] # [batch, hidden_state]\n\n        encoded_options = self.idiom_embedding(option_ids)\n        multiply_result = torch.einsum('abc,ac->abc', encoded_options, blank_states)\n\n        pooled_output = self.dropout(multiply_result)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.327492Z","iopub.execute_input":"2024-01-26T00:44:21.327816Z","iopub.status.idle":"2024-01-26T00:44:21.341499Z","shell.execute_reply.started":"2024-01-26T00:44:21.327790Z","shell.execute_reply":"2024-01-26T00:44:21.340441Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def read_query(query, choices, arr):\n    if arr == 0:\n        if len(re.findall(r'\\[BLANK\\d+\\]', query)) == 1:\n            tag = '[' + ''.join(re.findall(\"BLANK1\", query)) + ']'\n            tmp_context = query\n\n            label = None\n            options = choices\n\n            return ViidExample(\n                tag=tag,\n                context=tmp_context,\n                options=choices,\n                label=label\n            )\n        else: \n            examples = []\n            tags = re.findall(r'\\[BLANK\\d+\\]', query)\n            # print(tags)\n            # return\n            options = choices\n            idx = 0\n            for tag in tags:\n                tmp_context = query\n                for other_tag in tags:\n                    if other_tag != tag:\n                        tmp_context = tmp_context.replace(other_tag, \"[UNK]\")\n                label = None\n                examples.append(ViidExample(\n                    tag=tag,\n                    context=tmp_context,\n                    options=options,\n                    label=label\n                ))\n            return examples\n    else: \n        examples = []\n        for idx, q in enumerate(query):\n            tags = re.findall(r'\\[BLANK\\d+\\]', q)\n            options = choices[idx]\n            idx = 0\n            for tag in tags:\n                tmp_context = q\n                for other_tag in tags:\n                    if other_tag != tag:\n                        tmp_context = tmp_context.replace(other_tag, \"[UNK]\")\n                label = None\n                examples.append(ViidExample(\n                    tag=tag,\n                    context=tmp_context,\n                    options=options,\n                    label=label\n                ))\n        return examples\ndef convert_query_to_feature(query, tokenizer, max_seq_length, is_list):\n    if is_list == 0:\n        label = None\n        if query.label is not None:\n            label = query.label\n\n        tag = query.tag\n        context = query.context\n\n        parts = context.split(tag)\n\n        assert len(parts) == 2\n        before_part = tokenizer.tokenize(parts[0]) if len(parts[0]) > 0 else []\n        after_part = tokenizer.tokenize(parts[1]) if len(parts[1]) > 0 else []\n\n\n        half_length = int(max_seq_length / 2)\n        if len(before_part) < half_length: # cut at tail\n            st = 0\n            ed = min(len(before_part) + 1 + len(after_part), max_seq_length - 2)\n        elif len(after_part) < half_length: # cut at head\n            ed = len(before_part) + 1 + len(after_part)\n            st = max(0, ed - (max_seq_length - 2))\n        else: # cut at both sides\n            st = len(before_part) + 3 - half_length\n            ed = len(before_part) + 1 + half_length\n\n        option_ids = [idiom_vocab[get_choice(each)] for each in query.options]\n\n        tokens = before_part + [\"[MASK]\"] + after_part\n        tokens = [\"[CLS]\"] + tokens[st:ed] + [\"[SEP]\"]\n\n        position = tokens.index(\"[MASK]\")\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n        segment_ids = [0] * len(input_ids)\n\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        return InputFeatures(\n            tag=int(tag[6: -1]),\n            input_ids=input_ids,\n            input_mask=input_mask,\n            segment_ids=segment_ids,\n            option_ids=option_ids,\n            position=position,\n            label=label\n        )\n    else:\n        features = []\n        for example in query:\n\n            label = None\n            if example.label is not None:\n                label = example.label\n\n            tag = example.tag\n            context = example.context\n            parts = context.split(tag)\n            # print(parts)\n            assert len(parts) == 2\n            before_part = tokenizer.tokenize(parts[0]) if len(parts[0]) > 0 else []\n            after_part = tokenizer.tokenize(parts[1]) if len(parts[1]) > 0 else []\n\n            half_length = int(max_seq_length / 2)\n            if len(before_part) < half_length: # cut at tail\n                st = 0\n                ed = min(len(before_part) + 1 + len(after_part), max_seq_length - 2)\n            elif len(after_part) < half_length: # cut at head\n                ed = len(before_part) + 1 + len(after_part)\n                st = max(0, ed - (max_seq_length - 2))\n            else: # cut at both sides\n                st = len(before_part) + 3 - half_length\n                ed = len(before_part) + 1 + half_length\n\n            option_ids = [int(idiom_vocab[get_choice(each)]) for each in example.options]\n            # print(example.options)\n            tokens = before_part + [\"[MASK]\"] + after_part\n            tokens = [\"[CLS]\"] + tokens[st:ed] + [\"[SEP]\"]\n            position = tokens.index(\"[MASK]\")\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n            input_mask = [1] * len(input_ids)\n            segment_ids = [0] * len(input_ids)\n\n            padding = [0] * (max_seq_length - len(input_ids))\n            input_ids += padding\n            input_mask += padding\n            segment_ids += padding\n\n            features.append(InputFeatures(\n                tag=int(tag[6: -1]),\n                input_ids=input_ids,\n                input_mask=input_mask,\n                segment_ids=segment_ids,\n                option_ids=option_ids,\n                position=position,\n                label=label\n            ))\n\n        return features","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.343154Z","iopub.execute_input":"2024-01-26T00:44:21.343669Z","iopub.status.idle":"2024-01-26T00:44:21.383914Z","shell.execute_reply.started":"2024-01-26T00:44:21.343582Z","shell.execute_reply":"2024-01-26T00:44:21.382636Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def get_feature_for_pred(query, choices, is_list, arr):\n    input = read_query(query, choices, arr)\n    feature = convert_query_to_feature(input, tokenizer, max_seq_length, is_list)\n    if is_list == 0:\n        input_ids = torch.tensor([feature.input_ids], dtype=torch.long)\n        input_mask = torch.tensor([feature.input_mask], dtype=torch.long)\n        segment_ids = torch.tensor([feature.segment_ids], dtype=torch.long)\n        option_ids = torch.tensor([feature.option_ids], dtype=torch.long)\n        positions = torch.tensor([feature.position], dtype=torch.long)\n        tags= torch.tensor([feature.tag], dtype=torch.long)\n    \n        # eval_data = TensorDataset(input_ids, input_mask, segment_ids, option_ids, positions, tags)\n\n        model.eval()\n\n        input_ids = input_ids.to(device)\n        input_mask = input_mask.to(device)\n        segment_ids = segment_ids.to(device)\n        option_ids = option_ids.to(device)\n        positions = positions.to(device)\n\n        with torch.no_grad():\n            batch_logits = model(input_ids, option_ids, segment_ids, input_mask, positions)\n        answer = None\n        for i, tag in enumerate(tags):\n            logits = batch_logits[i].detach().cpu().numpy()\n            ans = np.argmax(logits)\n            answer = ans\n\n        return answer\n    else:\n        all_input_ids = torch.tensor([f.input_ids for f in feature], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in feature], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in feature], dtype=torch.long)\n        all_option_ids = torch.tensor([f.option_ids for f in feature], dtype=torch.long)\n        all_positions = torch.tensor([f.position for f in feature], dtype=torch.long)\n        all_tags= torch.tensor([f.tag for f in  feature],dtype=torch.long)\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_option_ids, all_positions, all_tags)\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=predict_batch_size)\n\n        model.eval()\n        all_results = {}\n        logger.info(\"Start evaluating\")\n        count = 0\n        for input_ids, input_mask, segment_ids, option_ids, positions, tags in \\\n                tqdm(eval_dataloader, desc=\"Evaluating\",disable=None):\n            if len(all_results) % 1000 == 0:\n                logger.info(\"Processing example: %d\" % (len(all_results)))\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n            option_ids = option_ids.to(device)\n            positions = positions.to(device)\n\n            with torch.no_grad():\n                batch_logits = model(input_ids, option_ids, segment_ids, input_mask, positions)\n            # print(len(tags))\n            for i, tag in enumerate(tags):\n                logits = batch_logits[i].detach().cpu().numpy()\n                ans = np.argmax(logits)\n                all_results[str(count)] = ans\n                count += 1\n        return all_results","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.386153Z","iopub.execute_input":"2024-01-26T00:44:21.386910Z","iopub.status.idle":"2024-01-26T00:44:21.407394Z","shell.execute_reply.started":"2024-01-26T00:44:21.386880Z","shell.execute_reply":"2024-01-26T00:44:21.406478Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"output_model_file = '/kaggle/input/model-demo/pytorch_model_distilbert-base-vi-cased_dtv3.bin'\npretrained_model = 'Geotrend/distilbert-base-vi-cased'\nbert_config_file = '/kaggle/working/distilbert-base-vi-cased/config.json'\ninit_checkpoint = '/kaggle/working/distilbert-base-vi-cased/pytorch_model.bin'\noutput_dir = './output_model'","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.408780Z","iopub.execute_input":"2024-01-26T00:44:21.409171Z","iopub.status.idle":"2024-01-26T00:44:21.423243Z","shell.execute_reply.started":"2024-01-26T00:44:21.409134Z","shell.execute_reply":"2024-01-26T00:44:21.422370Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"bert_config = BertConfig.from_json_file(bert_config_file)\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:21.424511Z","iopub.execute_input":"2024-01-26T00:44:21.424864Z","iopub.status.idle":"2024-01-26T00:44:22.426096Z","shell.execute_reply.started":"2024-01-26T00:44:21.424836Z","shell.execute_reply":"2024-01-26T00:44:22.425064Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01bf8eb976824cc3955e50e0c5cb1b65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"955be06d5c774326bbfc235572161bf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/104k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a455fd7c5c34222b6de611af0fe6a55"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"model_state_dict = torch.load(output_model_file)\nmodel = BertForCloze(bert_config, num_choices=7)\nmodel.load_state_dict(model_state_dict)\nmodel.to(device)\nif n_gpu > 1:\n    model = torch.nn.DataParallel(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:22.427435Z","iopub.execute_input":"2024-01-26T00:44:22.427760Z","iopub.status.idle":"2024-01-26T00:44:28.837003Z","shell.execute_reply.started":"2024-01-26T00:44:22.427732Z","shell.execute_reply":"2024-01-26T00:44:28.836110Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"context = 'Biết nhẫn nại và kiên trì, luôn luôn có lòng dũng cảm cao hơn kẻ thù, chúng ta sẽ đạt đến những điều tốt đẹp nhất, đó là những điều có giá trị lớn lao và mang tầm quan trọng đặc biệt đối với mỗi một cá nhân và chúng ta đều nhận thức được điều đó. Và câu tục ngữ “[BLANK1]” cho đến hôm nay vẫn luôn là một kinh nghiệm rèn luyện cá nhân vô cùng quý báu.'\nchoices = [     \"Ăn mày nơi cả thể, làm rể nơi nhiều con\",\n                \"Vẽ đường cho hươu chạy\",\n                \"Một miếng khi đói bằng một gói khi no\",\n                \"Ăn thì vóc, học thì hay\",\n                \"Có công mài sắt, có ngày nên kim\",\n                \"Giặc đến nhà, đàn bà cũng đánh\",\n                \"Trẻ trồng na, già trồng chuối\"\n            ]\nanswer = 4","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:28.838225Z","iopub.execute_input":"2024-01-26T00:44:28.838563Z","iopub.status.idle":"2024-01-26T00:44:28.845365Z","shell.execute_reply.started":"2024-01-26T00:44:28.838535Z","shell.execute_reply":"2024-01-26T00:44:28.844452Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"ans = get_feature_for_pred(context, choices, is_list = 0, arr = 0)\nprint(choices[ans])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:28.846526Z","iopub.execute_input":"2024-01-26T00:44:28.846850Z","iopub.status.idle":"2024-01-26T00:44:29.310911Z","shell.execute_reply.started":"2024-01-26T00:44:28.846821Z","shell.execute_reply":"2024-01-26T00:44:29.309823Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Có công mài sắt, có ngày nên kim\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"context = \"Từ xưa, ông cha ta đã quan niệm mục đích của việc học trước hết là để làm người, rồi sau đó là tham gia việc làng, việc nước:“Học là học để làm người/ Biết điều hơn thiệt biết lời thị phi”. Câu ca dao này có ngụ ý là muốn khuyên chúng ta phải học tập để trở thành người có ích và nên học những gì tốt chứ không nên học tập những cái xấu. Cho nên, dù khó khăn, gian khổ đến mấy, cha mẹ vẫn cố gắng cho con em mình đi học. Với lối so sánh, ví von mộc mạc, ông cha ta đã khẳng định vai trò quan trọng của việc học tập: “Người mà không học, khác gì đi đêm/ [BLANK3]”; “Ngọc kia chẳng giũa chẳng mài/ Cũng thành vô dụng cũng hoài ngọc đi” - ý nói nếu không học con người sẽ trở nên mù mờ, tăm tối, dẫu có là ngọc đi chăng nữa mà không được mài, được giũa thì ngọc ấy cũng chẳng có giá trị gì. Hay câu ca dao: “Học là học biết giữ giàng/ Biết điều nhân nghĩa biết đàng hiếu trung”để khuyên mỗi người chúng ta cần phải học tập những đạo lý, lễ nghĩa làm người. Hoặc câu:“Người không học, không có sự hiểu biết/ Trẻ mà không học, lớn không làm được việc gì”; “[BLANK2]”,đã cho thấy việc học không có bất cứ giới hạn nào, mà học tập vừa là trách nhiệm, vừa là quyền lợi, nó luôn đòi hỏi sự nỗ lực và quyết tâm cao của mỗi con người. Câu tục ngữ: “[BLANK1]”, ý muốn nhắc nhở chúng ta phải rèn luyện ý chí , nghị lực để vươn lên trong cuộc sống. \"\nchoices = [     \"Cú đói ăn con\",\n                \"Luyện mãi thành tài, miệt mài tất giỏi\",\n                \"Đồng tiền đi liền khúc ruột\",\n                \"Muốn biết phải hỏi, muốn giỏi phải học\",\n                \"Người không học như ngọc không mài\",\n                \"trời cũng chiều người\",\n                \"phải trái phân minh\"\n            ]\nanswer = [1,3,4]","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:29.312470Z","iopub.execute_input":"2024-01-26T00:44:29.312902Z","iopub.status.idle":"2024-01-26T00:44:29.320130Z","shell.execute_reply.started":"2024-01-26T00:44:29.312862Z","shell.execute_reply":"2024-01-26T00:44:29.319072Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"ans = get_feature_for_pred(context, choices, is_list = 1, arr = 0)\nfor a in ans:\n    print(choices[ans[a]])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:29.321585Z","iopub.execute_input":"2024-01-26T00:44:29.322007Z","iopub.status.idle":"2024-01-26T00:44:29.384689Z","shell.execute_reply.started":"2024-01-26T00:44:29.321971Z","shell.execute_reply":"2024-01-26T00:44:29.383542Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Người không học như ngọc không mài\nNgười không học như ngọc không mài\nNgười không học như ngọc không mài\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"context = [\"Tuy nhiên, bên cạnh những câu đề cao và ca ngợi đức tính tốt của con người, cũng có những câu thành ngữ phê phán các thói xấu như lười biếng, lười lao động như câu “[BLANK1]”. Câu thành ngữ như hồi chuông cảnh tỉnh con người mau chóng thay đổi tư duy tiêu cực để đưa cá nhân, cộng đồng hoàn thiện và phát triển hơn. Vì trong cuộc sống, giàu có và thành công không dành cho những kẻ lười nhác.\",\n          \"Ở nơi cần tới sự khe khắt, tỉ mẩn và chính xác cao độ mà đem con mắt [BLANK1] ra thì dễ đứt cước. Hàng xóm sống cạnh nhau thì có thể lấy chín bỏ làm mười làm phương châm sống, nhưng khi họp tổ dân phố thì phải loại chín bỏ làm mười ra cho sự nghiêm túc và công minh có đất tồn tại. Nhân viên an ninh hàng không mà dùng con mắt chín bỏ làm mười thì không ổn tí nào.\"]\nchoices = [[\n            \"Nói phải củ cải cũng nghe\",\n            \"Cầm cân nảy mực\",\n            \"Há miệng chờ sung\",\n            \"Đánh rắn phải đánh dập đầu\",\n            \"Ăn hết nhiều, chứ ở hết bao nhiêu\",\n            \"Muốn biết phải hỏi, muốn giỏi phải học\",\n            \"Không có lửa làm sao có khói\"\n            ],\n         [\n            \"Trẻ cậy cha, già cậy con\",\n            \"Lợn giò, bò bắp\",\n            \"Thứ nhất phạm phòng, thứ nhì lòng lợn\",\n            \"Chín bỏ làm mười\",\n            \"Học một biết mười\",\n            \"Không thầy đố mày làm nên\",\n            \"Hay học thì sang, hay làm thì có\"\n        ]   ]\n\nanswer = [2], [3]","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:29.386395Z","iopub.execute_input":"2024-01-26T00:44:29.386948Z","iopub.status.idle":"2024-01-26T00:44:29.396068Z","shell.execute_reply.started":"2024-01-26T00:44:29.386906Z","shell.execute_reply":"2024-01-26T00:44:29.394853Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"ans = get_feature_for_pred(context, choices, is_list = 1, arr = 1)\nfor idx, a in enumerate(ans):\n    print(choices[idx][ans[a]])\n# ans","metadata":{"execution":{"iopub.status.busy":"2024-01-26T00:44:29.397320Z","iopub.execute_input":"2024-01-26T00:44:29.397652Z","iopub.status.idle":"2024-01-26T00:44:29.442171Z","shell.execute_reply.started":"2024-01-26T00:44:29.397626Z","shell.execute_reply":"2024-01-26T00:44:29.440995Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Không có lửa làm sao có khói\nHọc một biết mười\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"context = [\"Đây là triết lý giáo dục lấy người thầy làm trung tâm, lời thầy là khuôn vàng thước ngọc\\\". Tuy nhiên, trong cách hiểu truyền thống thông thường về câu thành ngữ \\\"[BLANK2]\\\" thì \\\"Tôn sư\\\" không có nghĩa là đặt người thầy vào vị trí trung tâm của của việc truyền giảng. Nếu như thế là ta đã hiểu sang vấn đề kỹ thuật chuyên môn ngành sư phạm. Tôn sư là đề cao vai trò của người thầy, và người thầy luôn ở vị trí được tôn kính. Theo như một câu tục ngữ của người Việt Nam ta: \\\"[BLANK1]\\\". Bởi lẽ, trong giáo dục, nói một cách cụ thể là trong việc dạy và học, người học luôn đóng vai trò là trung tâm của sự hội tụ kiến thức. Theo đó, người thầy giáo phải đánh giá đúng đối tượng là người học, hiểu và nắm bắt suy nghĩ của người học, hiểu tâm lý và khả năng nhận thức của người học để từ đó xây dựng nội dung giảng dạy, lựa phương pháp truyền thụ cho thích hợp và định lượng nội dung giảng dạy. Mọi sự đánh giá chủ quan, phiến điện không đúng về đối tượng người học đều dẫn đến thất bại trong giáo dục. \\\"Tôn sư\\\" không nghĩa là thầy luôn luôn đúng, vì điều đó còn tùy thuộc vào sức khỏe, trạng thái tâm lý của thầy giáo, sự thay đổi của điều kiện ngoại cảnh mà đã tác động đến hoạt động giáo dục. \\\"Trọng đạo\\\" cũng có nghĩa là trọng chân lý, xét trong tình huống giáo dục cụ thể chân lý là của cùng một chủ thể thầy dạy đưa ra, nhưng cũng có khi là chân lý được học trò đúc kết, tích lũy trong hoạt động sống nói chung. Vì thế, trong giáo dục, học trò vẫn có thể tranh luận với thầy, phản biện lại thầy về kiến thức chân lý mà vẫn giữ nguyên đạo lý và sự tôn sư.  Như thế, sự \\\"tôn sư\\\" đi liền với \\\"trọng đạo\\\" không tách rời nhau mà luôn ở trong cùng một quan niệm nên không hề lỗi thời như quan điểm của tác giả Trương Khắc Trà trong bài viết. Trong bài viết, tác gỉả viết: \\\"Tiếc thay, chế độ phong kiến - Nho giáo đã hết vai trò lịch sử, đã lùi vào hậu trường nhưng tư tưởng giáo dục của nó vẫn hằn in trong cách nghĩ, cách học, cách làm giáo dục của người Việt trong thế kỷ XXI.   Ngày nay, \\\"tôn sư trọng đạo\\\" vẫn mang ý nghĩa tôn vinh người thầy và nghề dạy học, nhưng giáo dục ngày nay cơ bản đã khác xưa, mối quan hệ thầy trò cũng cũng phải vận động sao cho phù hợp với thời cuộc\\\". Ở đây, cho dù là sự vận động như thế nào đi chăng nữa thì vai trò của người thầy cũng không thể thay thế được. Người Trung Quốc có câu: \\\"Một gánh sách không bằng một người thầy giỏi\\\". Mặt khác, sự \\\"tôn sư\\\" ở đây còn được hiểu là kính trọng thầy về kiến thức và đạo đức, nhưng cũng còn ý nghĩa khác nữa là quý mến thầy trong cách hiểu về tình người. \",\n          \"Nhiều người cho rằng từ những thành ngữ trên, người Việt đã phát triển thành câu [BLANK2]. Tuy nhiên, theo chúng tôi, Nhất tự vi sư (一字為師) có khả năng là của Trung Quốc, vì đã từng xuất hiện trong bộ Trần Thị Liên Châu tập (陳氏聯珠集), quyển 10 của Thang Tín Đỗ (năm 1802). Ngoài ra, trong Hán ngữ còn có câu Nhất nhật vi sư, chung thân vi phụ (Một ngày cũng là thầy, suốt đời mới là cha). Chỉ có thành ngữ Bán tự vi sư (半字為師) mới có khả năng là của Việt Nam, kết hợp với thành ngữ Nhất tự vi sư của Trung Quốc để thành Nhất tự vi sư, bán tự vi sư ([BLANK1]).\"]\nchoices = [[\n                \"Nói nhăng nói cuội\",\n                \"Tôn sư trọng đạo\",\n                \"Đoàn kết thì sống, chia rẽ thì chết\",\n                \"Thứ nhất phạm phòng, thứ nhì lòng lợn\",\n                \"Cha sinh chẳng bằng mẹ dưỡng\",\n                \"Há miệng mắc quai\",\n                \"Không thầy đố mày làm nên\"\n            ],\n         [\n                \"Cá mè một lứa\",\n                \"Trẻ cậy cha, già cậy con\",\n                \"Miệng quan trôn trẻ\",\n                \"Nhất tự vi sư, bán tự vi sư\",\n                \"Một chữ cũng là thầy, nửa chữ cũng là thầy\",\n                \"Thật thà là cha quỷ quái\",\n                \"Sát nhân giả tử\"\n            ]]\n\nanswer = [6, 1], [4, 3]","metadata":{"execution":{"iopub.status.busy":"2024-01-26T01:27:03.510884Z","iopub.execute_input":"2024-01-26T01:27:03.511296Z","iopub.status.idle":"2024-01-26T01:27:03.522603Z","shell.execute_reply.started":"2024-01-26T01:27:03.511264Z","shell.execute_reply":"2024-01-26T01:27:03.521312Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"ans = get_feature_for_pred(context, choices, is_list = 1, arr = 1)\nl = []\nfor c in context:\n    l.append(len(re.findall(r'\\[BLANK\\d+\\]', c)))\ncount = 0\nflag = 0\nfor idx, a in enumerate(ans):\n    print(choices[flag][ans[a]])\n    count += 1\n    if count == l[flag]:\n        flag += 1\n        count = 0","metadata":{"execution":{"iopub.status.busy":"2024-01-26T01:27:03.880441Z","iopub.execute_input":"2024-01-26T01:27:03.880873Z","iopub.status.idle":"2024-01-26T01:27:03.946652Z","shell.execute_reply.started":"2024-01-26T01:27:03.880841Z","shell.execute_reply":"2024-01-26T01:27:03.945354Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Tôn sư trọng đạo\nTôn sư trọng đạo\nCá mè một lứa\nCá mè một lứa\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}